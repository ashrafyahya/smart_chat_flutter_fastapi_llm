# ğŸ§  LLM Chat Backend (FastAPI + GPT4All)

Ein leichtgewichtiges, vollstÃ¤ndig **offline** laufendes Backend fÃ¼r ein Smart-Chat-System mit einem lokalen LLM (Large Language Model), wie z.â€¯B. **GPT4All** oder **LLaMA (gguf)**.  
Ideal fÃ¼r Flutter-Apps oder andere Clients, die lokal mit einem Chatmodell kommunizieren sollen.

---

## ğŸ¯ Ziel

- Bereitstellung einer HTTP-API (`/chat`) zur Kommunikation mit einem lokalen KI-Modell.
- Offline-Nutzung ohne Internet.
- Schnelle Antworten durch quantisierte Modelle (gguf).
- Modularer Aufbau fÃ¼r spÃ¤tere Integration mit anderen Modellen (transformers, llama.cpp, etc.).

---

## ğŸ“ Projektstruktur

```plaintext
smart_chat_backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py          # FastAPI entrypoint
â”‚   â”œâ”€â”€ model.py         # Modellanbindung GPT4All / LLaMA
â”‚   â””â”€â”€ schemas.py       # Datenmodelle (Pydantic)
â”œâ”€â”€ requirements.txt     # Python-AbhÃ¤ngigkeiten
â””â”€â”€ README.md            # Dieses Dokument


ğŸš€ Installation
1. ğŸ“¦ Voraussetzungen

    Python 3.9+

    Windows, macOS oder Linux

    mind. 8 GB RAM empfohlen (je nach Modell)



2. ğŸ”§ Setup
# Projekt klonen oder neu anlegen
git clone <dieses-repo> smart_chat_backend
cd smart_chat_backend

# Virtuelle Umgebung erstellen
python -m venv .venv
.venv\Scripts\activate   # Windows

# AbhÃ¤ngigkeiten installieren
pip install -r requirements.txt


3. ğŸ§  Modell vorbereiten
GPT4All-Modell

    Lade ein Modell von https://gpt4all.io/models herunter.
    Beispiel: mistral-7b-openorca.Q4_0.gguf

    Platziere das Modell im GPT4All-Verzeichnis:
    C:\Users\<DeinBenutzer>\.gpt4all\models\
    oder passe den Pfad in app/model.py an:
    MODEL_PATH = "Pfad/zum/deinem/modell.gguf"


â–¶ï¸ Starten des Servers
uvicorn app.main:app --reload


LÃ¤uft auf:
ğŸ“ http://127.0.0.1:8000

Swagger-Doku (TestoberflÃ¤che):
ğŸ“„ http://127.0.0.1:8000/docs
---------------------------------------------------------------------
Server starten:
$python server.py


Anfrage an Server schicken:
curl -X POST "http://localhost:8000/generate" ^
     -H "Content-Type: application/json" ^
     -d "{\"prompt\": \"Was ist die Hauptstadt von Deutschland?\"}"


