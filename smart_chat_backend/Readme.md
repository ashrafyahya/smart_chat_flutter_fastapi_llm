# ğŸ§  LLM Chat Backend (FastAPI + llama.cpp)

Ein leichtgewichtiges, vollstÃ¤ndig **offline** laufendes Backend fÃ¼r ein Smart-Chat-System mit einem lokalen LLM (Large Language Model), z.â€¯B. **LLaMA (gguf)** Ã¼ber [llama.cpp](https://github.com/ggerganov/llama.cpp).  
Ideal fÃ¼r Flutter-Apps oder andere Clients, die lokal mit einem Chatmodell kommunizieren sollen.

---

## ğŸ¯ Ziel

- Bereitstellung einer HTTP-API (`/generate`) zur Kommunikation mit einem lokalen KI-Modell.
- Offline-Nutzung ohne Internet.
- Schnelle Antworten durch quantisierte Modelle (gguf).
- Modularer Aufbau fÃ¼r spÃ¤tere Integration mit anderen Modellen (transformers, llama.cpp, etc.).

---

## ğŸ“ Projektstruktur

```plaintext
smart_chat_backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ chat.py          # FastAPI entrypoint
â”‚   â”œâ”€â”€ model.py         # Modellanbindung llama.cpp
â”œâ”€â”€ config.py            # Projektweite Konfiguration (Modellpfad, Host, Port)
â”œâ”€â”€ requirements.txt     # Python-AbhÃ¤ngigkeiten
â”œâ”€â”€ server.py            # Startet den Server
â””â”€â”€ README.md            # Dieses Dokument
```

---

## ğŸš€ Installation

1. ğŸ“¦ Voraussetzungen

    - Python 3.9+
    - Windows, macOS oder Linux
    - mind. 8 GB RAM empfohlen (je nach Modell)

2. ğŸ”§ Setup

    ```sh
    # Projekt klonen oder neu anlegen
    git clone <dieses-repo> smart_chat_backend
    cd smart_chat_backend

    # Virtuelle Umgebung erstellen
    python -m venv .venv
    .venv\Scripts\activate   # Windows

    # AbhÃ¤ngigkeiten installieren
    pip install -r requirements.txt
    ```

3. ğŸ§  Modell vorbereiten

    - Lade ein GGUF-Modell von z.B. [TheBloke](https://huggingface.co/TheBloke) oder anderen Quellen herunter.
    - Beispiel: mistral-7b-openorca.Q4_0.gguf
    - Passe den Pfad in `config.py` an:
      ```python
      MODEL_PATH = "Pfad/zum/deinem/modell.gguf"
      ```

---

## â–¶ï¸ Starten des Servers

```sh
python server.py
```
Mit --reload: Server startet bei jeder Ã„nderung automatisch neu (nur fÃ¼r Entwicklung empfohlen)

LÃ¤uft auf:  
ğŸ“ http://127.0.0.1:8000

Swagger-Doku (TestoberflÃ¤che):  
ğŸ“„ http://127.0.0.1:8000/docs

---

## Anfrage an Server schicken

```sh
curl -X POST "http://localhost:8000/generate" 
    -H "Content-Type: application/json" 
    -d "{\"prompt\": \"Was ist die Hauptstadt von Deutschland?\"}"
```


